{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "## Normal Distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voters = Table().with_columns(\"Candidate\", [\"A\", \"B\"], \"Chance\", [0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://inferentialthinking.com/notebooks/baby.csv\n",
    "births = Table.read_table('baby.csv')\n",
    "births.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of them on separate histograms\n",
    "births.hist(overlay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maternal height looks pretty normal (height and weight often are)\n",
    "\n",
    "avg_h = np.round(np.mean(births.column('Maternal Height')), 1)\n",
    "sd_h = np.round(np.std(births.column(\"Maternal Height\")),1)\n",
    "print(\"mean:  \"+str(avg_h)+\"     standard deviation:  \"+ str(sd_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bell shaped curve\n",
    "births.hist(3, bins=np.arange(55.5, 72.5, 1), unit='inch')\n",
    "positions = np.arange(-3, 3.1, 1)* sd_h + avg_h\n",
    "plots.xticks(positions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The standard normal curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "plot_normal_cdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = []\n",
    "for i in np.arange(1000000):\n",
    "    samples.append( np.random.normal())\n",
    "    \n",
    "    \n",
    "sample_table = Table().with_column('Sample', samples)\n",
    "sample_table.hist(bins=np.arange(-3.5, 3.6, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Plus or Minus a Few SDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples are within 3 SDs?\n",
    "\n",
    "sample_table.where('Sample', are.not_below(-3)).where('Sample', are.not_above(3)).num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as a proportion of all samples\n",
    "sample_table.where('Sample', are.not_below(-3)).where('Sample', are.not_above(3)).num_rows/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chebyshev says within 3 SDs, at least 1-1/9 = 88.8888%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion within 2 SDs\n",
    "sample_table.where('Sample', are.not_below(-2)).where('Sample', are.not_above(2)).num_rows/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion within 1 SD\n",
    "sample_table.where('Sample', are.not_below(-1)).where('Sample', are.not_above(1)).num_rows/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aren't Normal Distributions Rare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maternal age, not bell shaped; has long tail\n",
    "births.hist(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"population mean:  \"+str(np.mean(births.column(2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"population mean:  \"+str(np.mean(births.sample(1000).column(2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in np.arange(10000):\n",
    "    means.append(np.mean(births.sample(1000).column(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_column('Mean', means).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://inferentialthinking.com/notebooks/united_summer2015.csv\n",
    "united = Table.read_table('united_summer2015.csv')\n",
    "united"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united.hist('Delay', bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_delay = np.mean(united.column('Delay'))\n",
    "sd_delay = np.std(united.column('Delay'))\n",
    "\n",
    "print(\"mean delay:  \"+str(mean_delay)+\"           standard deviation:  \"+str(sd_delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = united.select('Delay')\n",
    "delay.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note: look at how I build my list of medians and how I use append method. A lot of questions on piazza about it. \n",
    "\n",
    "means = []\n",
    "for i in np.arange(10000):\n",
    "    sample = delay.sample(400)\n",
    "    means.append(np.mean(sample.column(0)))\n",
    "\n",
    "Table().with_column('Sample mean', means).hist(bins=30, unit='minute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What will happen if we change the sample size from 400 to 10?\n",
    "# A: About the same (bell curve)\n",
    "# B: More jagged \n",
    "# C: Longer left tail\n",
    "# D: Longer right tail\n",
    "# E: More like the histogram of delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in np.arange(10000):\n",
    "    sample = delay.sample(10)\n",
    "    means.append(np.mean(sample.column(0)))\n",
    "\n",
    "Table().with_column('Sample mean', means).hist(bins=30, unit='minute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(#back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability of the sample mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function\n",
    "# parameter: sample size\n",
    "\n",
    "def sample_means(sample_size):\n",
    "    means = []\n",
    "    for i in np.arange(10000):\n",
    "        sample = delay.sample(sample_size)\n",
    "        means.append(np.mean(sample.column(0)))\n",
    "    return means\n",
    "sample_means(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare different sample sizes\n",
    "Table().with_column(\n",
    "    '400', sample_means(400),\n",
    "    '900', sample_means(900),\n",
    "    '2500', sample_means(2500),\n",
    ").hist(bins=30, unit='minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how narrow do they get and why is it important\n",
    "\n",
    "# back to slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variability(sample_size):\n",
    "    means = sample_means(sample_size)\n",
    "    Table().with_column('Sample mean', means).hist(bins=30, unit='minute')\n",
    "    \n",
    "    print('Sample size:          ', sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variability(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# according to the formula: SD = (population SD) / √sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variability(sample_size):\n",
    "    means = sample_means(sample_size)\n",
    "    Table().with_column('Sample mean', means).hist(bins=30, unit='minute')\n",
    "    sqrt_n = np.sqrt(sample_size)\n",
    "\n",
    "    print('Sample size:                   ', sample_size)\n",
    "    print('Square root n:                 ', sqrt_n)\n",
    "    print('Sample mean SD:                ', np.std(means))\n",
    "    print('Population SD / Square root n =', sd_delay / sqrt_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variability(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's increase the sample size\n",
    "variability(800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What SD do you expect for sample size 3200?\n",
    "\n",
    "# A: Half of SD for sample size 800\n",
    "# B: Same as SD for sample size 800\n",
    "# C: Double SD for sample size 800\n",
    "# D: Four times SD for sample size 800\n",
    "# E: I don't know\n",
    "\n",
    "variability(3200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variability(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if I change sample size to 2?\n",
    "variability(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how the histogram becomes more normal as sample size increases\n",
    "\n",
    "for i in 2**(np.arange(10)):\n",
    "    variability(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals and Sample Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://inferentialthinking.com/notebooks/san_francisco_2015.csv\n",
    "sf = Table.read_table('san_francisco_2015.csv').select(3, 11, 21)\n",
    "sf.set_format(2, NumberFormatter(0))\n",
    "sf = sf.where(2, are.above(10000))\n",
    "sf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_bins = np.arange(0, 700000, 25000)\n",
    "sf.hist(2, bins=comp_bins, unit='dollar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confidence interval methods\n",
    "\n",
    "def bootstrap_mean(sample_from_population, label, repetitions):\n",
    "    resampled_means = []\n",
    "    for i in np.arange(repetitions):\n",
    "        resample = sample_from_population.sample()\n",
    "        mean = np.mean(resample.column(label))\n",
    "        resampled_means.append(mean)\n",
    "    return resampled_means\n",
    "\n",
    "def bootstrap_ci_mean(sample_from_population, label, repetitions):\n",
    "    resampled_means = bootstrap_mean(sample_from_population, label, repetitions)\n",
    "    \n",
    "    interval_95 = make_array(\n",
    "        percentile(2.5, resampled_means),\n",
    "        percentile(97.5, resampled_means)\n",
    "    )\n",
    "    \n",
    "    Table().with_column('Resampled mean', resampled_means).hist(0)\n",
    "    plots.plot(interval_95, [0, 0], color='gold', lw=8)\n",
    "    print('Approximate 95% Bootstrap Confidence Interval for Population Mean:')\n",
    "    print(np.round(interval_95, 3))\n",
    "    print('Interval Width: '+str(interval_95[1] - interval_95[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_sample = sf.sample(200)\n",
    "sf_sample.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample 1000 times from my original sample (with replacement)\n",
    "#display histogram showing 95% confidence interval\n",
    "\n",
    "bootstrap_ci_mean(sf_sample, 'Total Compensation', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size was 200. Confidence interval is too wide\n",
    "# Note, numbers will change because samples are random! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What do you think the sample size should be to achieve a \n",
    "# 95% confidence interval with width of $10,000 or less?\n",
    "\n",
    "# A: 200\n",
    "# B: 300\n",
    "# C: 400\n",
    "# D: 800\n",
    "# E: 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sf_sample = sf.sample(800)\n",
    "bootstrap_ci_mean(sf_sample, 'Total Compensation', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(back to slides to recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential population\n",
    "\n",
    "votes = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "np.std(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "np.std(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "np.std(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_voters=10\n",
    "\n",
    "def sd_voters(n_voters_for_a):\n",
    "    votes = np.append(np.ones(n_voters_for_a), np.zeros(total_voters - n_voters_for_a))\n",
    "    print(votes)\n",
    "    return np.std(votes)\n",
    "\n",
    "sd_voters(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fact: if data contains only 0-1 then SD will never be above 0.5\n",
    "# we know that worst_sd_pop = 0.5\n",
    "# width_in_sds = 4\n",
    "# desired_width = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √(sample size)  ≥  4 x (SD of 0-1 population) / 0.03\n",
    "worst_sd_pop = 0.5\n",
    "width_in_sds = 4\n",
    "desired_width = 0.03\n",
    "(width_in_sds * (worst_sd_pop / desired_width)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  no matter what how my population looks like, if I take a sample of 4445 people, my CI will always be 0.03 width or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters.select(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_sample = voters.select(0).sample(1000, weights=voters.column('Chance'))\n",
    "observed_sample.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4445\n",
    "observed_sample = voters.select(0).sample(n, weights=voters.column('Chance'))\n",
    "\n",
    "means = []\n",
    "for i in np.arange(1000):\n",
    "    resample = observed_sample.sample()\n",
    "    means.append(np.count_nonzero(resample.column(0) == 'A') / n)\n",
    "print(percentile(2.5, means), percentile(97.5, means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6098987626546681 - 0.5808773903262092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
